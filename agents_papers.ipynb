{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JTmQi0xvsHQ"
      },
      "source": [
        "## this notebook was developed on Google Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSzPn-VCzYEv"
      },
      "source": [
        "### dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vAUf5Be4VA53"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3RlmhBmrDom"
      },
      "source": [
        "### data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "63zKOlrFC4X9"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "extending_context_window_llama_3 = \"https://arxiv.org/pdf/2404.19553\"\n",
        "attention_is_all_you_need = \"https://arxiv.org/pdf/1706.03762\"\n",
        "\n",
        "docs = PyMuPDFLoader(extending_context_window_llama_3).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "S5XI401hC59a"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode(\n",
        "        text,\n",
        "    )\n",
        "    return len(tokens)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = tiktoken_len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cTHvVilC7cR",
        "outputId": "1f097cce-9b9f-41cd-8171-3db9cb8ebe09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrNGjNF_DDVT"
      },
      "source": [
        "### embeddings and vectordb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1wKTkkeQDFpp"
      },
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SzO5TEXmDGuN"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vectorstore = Qdrant.from_documents(\n",
        "    split_chunks,\n",
        "    embedding_model,\n",
        "    location=\":memory:\",\n",
        "    collection_name='ExtendingLlama-3’sContext',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1sJoTiIYDH2B"
      },
      "outputs": [],
      "source": [
        "qdrant_retriever = qdrant_vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5wzdFF9DY_9"
      },
      "source": [
        "### rag chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "G9sFAl9JDPxN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "You are a helpful assistant. Use the available context to answer the question. If you can't answer the question, say you don't know.\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "qNT2SnQODV6L"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oV4Nit_vDXOS"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | qdrant_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | openai_chat_model | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bMCa7uQPDeUJ",
        "outputId": "e6bfff20-e2ac-409e-90b9-ac41ef7c2948"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"In the context of 'long context', the term 'context' refers to a coherent text, such as a book or a long paper.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"question\" : \"What does the 'context' in 'long context' refer to?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "naxWppQnOpjv",
        "outputId": "7265ff44-eeb5-4de3-f924-0e853074a898"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Extending Llama-3’s Context'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paper_title = rag_chain.invoke({\"question\" : \"Retrieve the paper title as string without any adittional word.\"})\n",
        "paper_title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sufiMocGrFbK"
      },
      "source": [
        "### tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "huI6-KcmcQY8"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "def write_markdown_file(content: Annotated[str, \"The string final content to write to the file.\"], filename: Annotated[str, \"The filename to save the file as, must be the paper title. do NOT include .md\"]):\n",
        "  \"\"\"Writes the given content as a markdown file to the local directory.\n",
        "\n",
        "  Args:\n",
        "    content: The string content to write to the file.\n",
        "    filename: The filename to save the file as, must be the paper title, do NOT include .md.\n",
        "  \"\"\"\n",
        "  with open(f\"documents/{filename}.md\", \"w\") as f:\n",
        "    f.write(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "D16RapGFrN2B"
      },
      "outputs": [],
      "source": [
        "def retrieve_paper_title():\n",
        "  \"\"\"Returns the paper title to use later\"\"\"\n",
        "  return rag_chain.invoke({\"question\" : \"Retrieve the paper title as string without any adittional word.\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jsdb3UQ1MNFi"
      },
      "outputs": [],
      "source": [
        "def retrieve_information(query: Annotated[str, \"query to ask the retrieve information tool\"], paper_title: Annotated[str, 'title of the paper']):\n",
        "    f\"\"\"Use Retrieval Augmented Generation to retrieve information about {paper_title} paper.\"\"\"\n",
        "    return rag_chain.invoke({\"question\" : query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1GMuuRH1pQlj"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import StructuredTool\n",
        "\n",
        "retrieve_paper_title = StructuredTool.from_function(\n",
        "                func=retrieve_paper_title,\n",
        "                name='RetrievePaperTitle',\n",
        "                description=\"retrieve the paper's title\")\n",
        "\n",
        "retrieve_information = StructuredTool.from_function(\n",
        "                func= retrieve_information,\n",
        "                name= 'RetrieveInformation',\n",
        "                description= 'search for information about the given paper using Retrieval Augmented Generation.',)\n",
        "\n",
        "write_content = StructuredTool.from_function(\n",
        "                func=write_markdown_file,\n",
        "                name=\"WriteContent\",\n",
        "                description=\"write the final content as a markdown file to the local directory.\",)\n",
        "\n",
        "\n",
        "tools = [retrieve_paper_title, retrieve_information, write_content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-iIp3L60Fg5v"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the retrieve_information tool for information.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5NgbGy1Eo5RT"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)#, handle_parsing_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAV2zdI8ul1T",
        "outputId": "d50857c4-573a-49cf-bc60-4159a3d435e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrievePaperTitle` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mThe paper title is \"Extending Llama-3’s Context\".\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'research problem and motivation of the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe research problem and motivation of the paper \"Extending Llama-3’s Context\" involve extending the context length of the Llama-3-8B-Instruct model from 8K to 80K through QLoRA fine-tuning. The goal is to improve the model's performance across various evaluation tasks, such as NIHS, topic retrieval, and long-context language understanding, while still maintaining its original capability over short contexts. The paper highlights the potential of LLMs to extend context length and aims to publicly release the resources for further research and development.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'claimed contributions and novelties of the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe claimed contributions and novelties of the paper \"Extending Llama-3’s Context\" include the extension of the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning, the super efficient training cycle taking only 8 hours on one 8xA800 (80G) GPU machine, superior performances across various evaluation tasks such as NIHS and topic retrieval, the ability to extend the context length beyond 80K with more computation resources, and the release of the entire resources including data, model, data generation pipeline, and training code to the public. Additionally, the paper highlights the preservation of the model's original capability over short contexts despite the dramatic context extension.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'methodology and substantiation of claims in the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe paper \"Extending Llama-3’s Context\" discusses extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning. The authors mention that the entire training cycle is super efficient, taking 8 hours on one 8xA800 (80G) GPU machine. They also highlight that the dramatic context extension is mainly attributed to merely 3.5K synthetic training samples generated by GPT-4. Additionally, they mention that the model exhibits superior performances across various evaluation tasks and that they will publicly release all resources related to the project. The methodology involves using QLoRA for efficient fine-tuning, applying LoRA on all Q,K,V,O projections, training the embedding layer, setting specific parameters for learning rate, batch size, and expansion of RoPE base. The claims made in the paper are substantiated through comparisons with baselines, showcasing the model's performance superiority despite the potential compromise on short-context capability.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'main conclusions and lessons learned from the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe main conclusions from the paper \"Extending Llama-3’s Context\" include the successful extension of the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA fine-tuning. The extended model exhibited superior performance across various evaluation tasks such as NIHS, topic retrieval, and long-context language understanding. It was noted that the context extension could compromise the model's short-context capability, but the overall performance was still superior to other open-source models at the same scale. The paper also highlighted the potential for further extending the context length beyond 80K with more computational resources. Additionally, the paper mentioned the release of resources including data, model, data generation pipeline, and training code to the public.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'significance of research problem in the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe significance of the research problem in the paper \"Extending Llama-3’s Context\" lies in the efficient extension of the context length of Llama-3-8B-Instruct from 8K to 80K using GPT-4 fine-tuning. This extension allows for superior performance in various evaluation tasks such as NIHS, topic retrieval, and long-context language understanding, while still preserving the original capability over short contexts. The research demonstrates the potential of large language models to extend their context length significantly with relatively few synthetic training samples, paving the way for further advancements in the field of long-context capabilities for language models.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'validity of claims and arguments in the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mBased on the provided context, the paper \"Extending Llama-3’s Context\" presents the extension of the context length of Llama-3-8B-Instruct from 8K to 80K using QLoRA fine-tuning. The paper claims that the extended model exhibits superior performances across various evaluation tasks and that the context length could be extended even further with more computational resources. Additionally, the paper acknowledges that extending the context length may compromise the model's short-context capability but states that their model's performance is still superior to other open-source models at the same scale.\n",
            "\n",
            "Therefore, based on the information provided in the context, the claims and arguments in the paper regarding the extension of Llama-3's context appear to be supported by their findings and evaluation results.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'core research problem and alternative approaches in the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe core research problem addressed in the paper \"Extending Llama-3’s Context\" is focused on extending the context length of Llama-3-8B-Instruct from 8K to 80K in an efficient manner. The alternative approach proposed in the paper involves using GPT-4 to synthesize 3.5K long-context training data for different long-context tasks, such as Single-Detail QA and Multi-Detail QA. The paper also mentions that other approaches to establishing long-context capabilities for large language models require significant compute and resources to accomplish.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'open problems for further research in the paper Extending Llama-3’s Context', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mOne potential open problem for further research in the paper \"Extending Llama-3’s Context\" could be exploring the impact of even further extending the context length beyond 80K with additional computational resources. This could involve investigating how the model's performance scales with increasingly longer contexts and whether there are diminishing returns or new challenges that arise with extremely long contexts.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `WriteContent` with `{'content': \"## Extending Llama-3’s Context\\n\\n### Comprehension:\\n\\n- **Research Problem and Motivation:**\\n  The research problem addressed in the paper involves extending the context length of the Llama-3-8B-Instruct model from 8K to 80K through QLoRA fine-tuning. The motivation is to enhance the model's performance across various evaluation tasks while maintaining its original capability over short contexts.\\n\\n- **Claimed Contributions and Novelties:**\\n  The paper claims contributions such as extending the context length to 80K, efficient training cycle, superior performances in evaluation tasks, potential for further extension, and public release of resources. The novelty lies in the context extension and performance preservation.\\n\\n- **Substantiation of Claims:**\\n  The claims are supported by the methodology involving QLoRA fine-tuning, efficient training, and comparisons with baselines to showcase performance superiority.\\n\\n- **Main Conclusions and Lessons Learned:**\\n  The main conclusions include successful context extension, superior performance in evaluation tasks, potential for further extension, and resource release.\\n\\n### Evaluation:\\n\\n- **Significance of Research Problem:**\\n  The research problem is significant as it demonstrates the potential of large language models to extend context length efficiently.\\n\\n- **Significance and Novelty of Contributions:**\\n  The contributions are significant and novel, especially in the context of extending context length and maintaining performance.\\n\\n- **Validity of Claims and Arguments:**\\n  The claims and arguments appear valid based on the findings and evaluation results.\\n\\n### Synthesis:\\n\\n- **Core Research Problem:**\\n  The core problem is extending the context length efficiently, with an alternative approach involving GPT-4 for data synthesis.\\n\\n- **Alternative Approaches and Substantiation:**\\n  Other approaches may require significant resources, highlighting the efficiency of the proposed method.\\n\\n- **Strengthening and Application of Results:**\\n  Further research could explore extending the context length beyond 80K with additional resources.\\n\\n- **Open Problems for Further Research:**\\n  One open problem could be investigating the impact of further extending the context length beyond 80K.\\n\", 'filename': 'Extending Llama-3’s Context.md'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mNone\u001b[0m\u001b[32;1m\u001b[1;3mI have created the review based on the information retrieved from the paper \"Extending Llama-3’s Context\". You can download the review from the following link: [Download Review](sandbox:/Extending%20Llama-3’s%20Context.md)\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "output = agent_executor.invoke(\n",
        "    {\n",
        "      \"input\": \"\"\"Identify and answer the following questions based on the paper content and reasearch:\n",
        "        -What is the research problem the paper attempts to address? What is the motivation?\n",
        "        -What are the claimed contributions and novelties of the paper?\n",
        "        -How do the authors substantiate their claims (methodology, experiments, proofs etc.)?\n",
        "        -What are the main conclusions and lessons learned?\n",
        "        -Is the research problem significant and not artificial?\n",
        "        -Are the claimed contributions really significant and novel compared to existing work?\n",
        "        -Are the claims and arguments valid, or are there flaws in the methodology, proofs, experiments etc.?\n",
        "        -What is the core of the research problem? Are there alternative approaches?\n",
        "        -Are there different ways to substantiate or argue against the claims?\n",
        "        -Can the results be strengthened or applied to other contexts?\n",
        "        -What are the open problems raised that could lead to further research?\n",
        "\n",
        "        Then, based on the questions and answers, use and complete the following template to write a review of the paper:\n",
        "\n",
        "        ## ReplaceThisWithPaperTitle\n",
        "\n",
        "        ### Comprehension:\n",
        "        \\n- **Research Problem and Motivation:**\n",
        "        \\n- **Claimed Contributions and Novelties:**\n",
        "        \\n- **Substantiation of Claims:**\n",
        "        \\n- **Main Conclusions and Lessons Learned:**\n",
        "\n",
        "        ### Evaluation:\n",
        "        \\n- **Significance of Research Problem:**\n",
        "        \\n- **Significance and Novelty of Contributions:**\n",
        "        \\n- **Validity of Claims and Arguments:**\n",
        "\n",
        "        ### Synthesis:\n",
        "        \\n- **Core Research Problem:**\n",
        "        \\n- **Alternative Approaches and Substantiation:**\n",
        "        \\n- **Strengthening and Application of Results:**\n",
        "        \\n- **Open Problems for Further Research:**\n",
        "\n",
        "        The review should be written in markdown format and saved as a file with the paper title as the filename.\n",
        "        \"\"\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCd6WFnMbZfv",
        "outputId": "48994871-4eb7-47a7-dd8b-2c371290abee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have created the review based on the information retrieved from the paper \"Extending Llama-3’s Context\". You can download the review from the following link: [Download Review](sandbox:/Extending%20Llama-3’s%20Context.md)\n"
          ]
        }
      ],
      "source": [
        "print(output['output'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
